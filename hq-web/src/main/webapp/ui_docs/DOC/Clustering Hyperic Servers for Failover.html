<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
    <head>
        <title>vFabric Hyperic 4.6.6 : Clustering Hyperic Servers for Failover</title>
	    <link rel="stylesheet" href="styles/site.css" type="text/css" />
        <META http-equiv="Content-Type" content="text/html; charset=UTF-8">	    
    </head>

    <body>
<b><a href="vFabric Hyperic 4.6.6.html" title="vFabric Hyperic 4.6.6 Documentation Home">vFabric Hyperic 4.6.6 Documentation Home (Internal)</a>
 - <a href="https://www.vmware.com/support/pubs/vfabric-hyperic.html">vFabric Hyperic Documentation Home (Online)</a>
 -  <a href="https://www.vmware.com/support/vfabric-hyperic/doc/vfabric-hyperic-rn-4.6.6.html">Hyperic 4.6.6 Release Notes</a> </b>
 <p>
 <p>
	    <table class="pagecontent" border="0" cellpadding="0" cellspacing="0" width="100%" bgcolor="#ffffff">
		    <tr>
			    <td valign="top" class="pagebody">
				    <div class="pageheader">
					    <span class="pagetitle">
                            vFabric Hyperic 4.6.6 : Clustering Hyperic Servers for Failover
                                                    </span>
				    </div>
				    <div class="pagesubheading">
					    This page last changed on Jan 04, 2012 by <font color="#0050B2">mmcgarry</font>.
				    </div>

				    <p><b>Available only in <font color="red">vFabric Hyperic</font></b></p>

<style type='text/css'>/*<![CDATA[*/
div.rbtoc1325699613261 {margin-left: 1.5em;padding: 0px;}
div.rbtoc1325699613261 ul {list-style: disc;margin-left: 0px;padding-left: 20px;}
div.rbtoc1325699613261 li {margin-left: 0px;padding-left: 0px;}

/*]]>*/</style><div class='rbtoc1325699613261'>
<ul>
    <li><a href='#ClusteringHypericServersforFailover-Overview'>Overview</a></li>
    <li><a href='#ClusteringHypericServersforFailover-RequirementsforanFailoverDeployment'>Requirements for an Failover Deployment</a></li>
    <li><a href='#ClusteringHypericServersforFailover-ConfiguringaServerCluster'>Configuring a Server Cluster</a></li>
<ul>
    <li><a href='#ClusteringHypericServersforFailover-Step1InstalltheFirstHypericServerInstance'>Step 1 - Install the First Hyperic Server Instance</a></li>
    <li><a href='#ClusteringHypericServersforFailover-Step2InstallAdditionalHypericServerNodes'>Step 2 - Install Additional Hyperic Server Nodes</a></li>
    <li><a href='#ClusteringHypericServersforFailover-Step3ConfigureClusterNameandCommunicationsProperties'>Step 3 - Configure Cluster Name and Communications Properties</a></li>
    <li><a href='#ClusteringHypericServersforFailover-Step4ConfiguretheLoadBalancer'>Step 4 - Configure the Load Balancer</a></li>
    <li><a href='#ClusteringHypericServersforFailover-Step5ConfigureAgentstoCommunicatewithHypericServerCluster'>Step 5 - Configure Agents to Communicate with Hyperic Server Cluster</a></li>
    <li><a href='#ClusteringHypericServersforFailover-Step6StarttheNodes'>Step 6 - Start the Nodes</a></li>
    <li><a href='#ClusteringHypericServersforFailover-TroubleshootingaFailoverConfiguration'>Troubleshooting a Failover Configuration</a></li>
</ul>
</ul></div>


<h1><a name="ClusteringHypericServersforFailover-Overview"></a>Overview</h1>

<p>To avoid interruption of Hyperic Server operation in the case of failure, you can configure a cluster of Hyperic Servers. The failover configuration uses:</p>
<ul>
	<li>EHCache's distributed caching for replicating changes throughout the cluster.</li>
</ul>


<ul>
	<li>The <tt>nodeStatus.hqu</tt> plugin for monitoring the availability of nodes.</li>
</ul>


<ul>
	<li>A hardware load balancer for managing failover when an node becomes unavailable. The load balancer checks the status of each node every 10 seconds, by issuing an HTTP request to the node's&nbsp;<tt>nodeStatus.hqu</tt> plugin. The check will return a response of <tt>master=true</tt> for the primary node, and a response of <tt>master=false</tt> for other nodes in the cluster.</li>
</ul>


<p>A Hyperic Server cluster contains multiple nodes; two are generally sufficient. One Hyperic Server, automatically selected by Hyperic, serves as the primary node. The other node or nodes serve as hot backups---they do not share the workload with the primary node.</p>

<p>A failover configuration is transparent to users and Hyperic administrators; it is not apparent that the active Hyperic server instance is clustered, or which node is currently active.</p>

<h1><a name="ClusteringHypericServersforFailover-RequirementsforanFailoverDeployment"></a>Requirements for an Failover Deployment</h1>

<ul>
	<li>A hardware-based load balancer.</li>
</ul>


<ul>
	<li>Only one Hyperic Server in an Hyperic Server cluster should receive agent communications at a time. The load balancer should should not direct agent connections to an Hyperic server instance that serves as the secondary node.</li>
</ul>


<ul>
	<li>Database Considerations---All nodes in the Hyperic cluster must share the same database. You cannot use Hyperic's internal PostgreSQL database in a failover configuration. You must use an external database; MySQL, Oracle, and PostgreSQL are supported.</li>
</ul>


<h1><a name="ClusteringHypericServersforFailover-ConfiguringaServerCluster"></a>Configuring a Server Cluster</h1>

<p>These instructions assume that you do not already have an Hyperic Server installation.</p>

<h2><a name="ClusteringHypericServersforFailover-Step1InstalltheFirstHypericServerInstance"></a>Step 1 - Install the First Hyperic Server Instance</h2>

<p>Run the full installer in the appropriate mode for the type of database server you will use (-mysql, &#45;postgresql, or &#45;oracle).&nbsp; You <b>must</b> choose one of these options, because clustering requires the use of an external Hyperic database. The installer will create the Hyperic database schema.</p>

<h2><a name="ClusteringHypericServersforFailover-Step2InstallAdditionalHypericServerNodes"></a>Step 2 - Install Additional Hyperic Server Nodes</h2>

<p>For each additional node:</p>
<ul>
	<li>Run the full installer in the appropriate mode for the type of database created during installation of the first server instance (-mysql, &#45;postgresql, or &#45;oracle).&nbsp;</li>
</ul>


<ul>
	<li>When the installer installer prompts for the location of the Hyperic database, specify the location of the database created for the first server instance.</li>
</ul>


<ul>
	<li>When the installer asks if you want to upgrade, overwrite, or exit the process, select the choice for "upgrade".</li>
</ul>


<h2><a name="ClusteringHypericServersforFailover-Step3ConfigureClusterNameandCommunicationsProperties"></a>Step 3 - Configure Cluster Name and Communications Properties</h2>

<p><a name="ClusteringHypericServersforFailover-FailoverProperties"></a><br/>
Configure the cluster-related properties on each of the Hyperic Servers in the cluster, in the "Cluster Settings" section of its <tt>conf/hq-server.conf</tt> file.</p>

<h3><a name="ClusteringHypericServersforFailover-Defaulthqserver.confFile"></a>Default hq-server.conf File</h3>

<div class="code panel" style="border-width: 1px;"><div class="codeContent panelContent">
<div id="root">
		<pre class="theme: Confluence; brush: java; gutter: false"># Cluster Settings
################################################################################
#
# Property: ha.partition
# &amp;nbsp;
# This property defines the name of the HQ cluster. Each HQ server with the
# same ha.partition name will join the same cluster. This property is required
# for proper cluster initialization.
#
#ha.partition=

#
# Property: ha.node.address
#
# This property defines the IP address or hostname to bind the multicast listener
# to. This property is required for proper cluster initialization.
#
#ha.node.address=

#
# Property: ha.node.mcast_addr
#
# This property defines the multicast address to use. This property is not required
# and defaults to 238.1.2.3.
#
#ha.node.mcast_addr=238.1.2.3

#
# Property ha.node.mcast_port
#
# This property defines the multicast port to use. This property is not required
# and defaults to 45566.
#
#ha.node.mcast_port=45566

#
# Property ha.node.cacheListener.port
#
# This property defines the multicast port that is used to discover cache peers. This
# property is not required and defaults to 45567
#ha.node.cacheListener.port=45567

#
# Property ha.node.cacheProvider.port
#
# This property defines the multicast port that is used to synchronize caches throughout
# the HQ cluster. This property is not required and defaults to 45568.
#ha.node.cacheProvider.port=45568</pre>
		</div>
</div></div>

<h3><a name="ClusteringHypericServersforFailover-RequiredClusterProperties"></a>Required Cluster Properties</h3>

<p>For each Hyperic Server in the cluster you must specify:</p>
<div class='table-wrap'>
<table class='confluenceTable'><tbody>
<tr>
<td class='confluenceTd'> ha.partition </td>
<td class='confluenceTd'> Name of the cluster---this value is identical for each node in the cluster </td>
</tr>
<tr>
<td class='confluenceTd'> ha.node.address </td>
<td class='confluenceTd'> Multicast listen address---specifies IP address or hostname upon which the node listens for multicast traffic; this value is unique to each node in the cluster. </td>
</tr>
</tbody></table>
</div>

<p><b>Note:</b> If you are upgrading from a pre-v3.0 failover configuration, the each server's .conf file will contain obsolete cluster properties, including server.cluster.mode and server.ha.bind_addr properties. Delete these properties and replace with the current failover properties described below.</p>

<h3><a name="ClusteringHypericServersforFailover-OptionalClusterProperties"></a>Optional Cluster Properties</h3>

<p>If desired, you can control these communication behaviors for the nodes in the cluster:</p>
<div class='table-wrap'>
<table class='confluenceTable'><tbody>
<tr>
<td class='confluenceTd'> ha.node.mcast_addr <br class="atl-forced-newline" />
and <br class="atl-forced-newline" />
ha.node.mcast_port </td>
<td class='confluenceTd'> Address and port for sending multicast messages to other nodes. Note: ha.node.mcast_addr must be the same on each node. </td>
</tr>
<tr>
<td class='confluenceTd'> ha.node.cacheListener.port <br class="atl-forced-newline" />
and ha.node.cacheProvider.port </td>
<td class='confluenceTd'> Ports used for discovering and synchronizing with cache peers. </td>
</tr>
</tbody></table>
</div>


<h2><a name="ClusteringHypericServersforFailover-Step4ConfiguretheLoadBalancer"></a>Step 4 - Configure the Load Balancer</h2>

<p>Configure the load balancer, according to the vendor or supplier instructions. Procedures vary, but at a minimum you will identify the Hyperic Server nodes in the cluster and the failover behavior. </p>

<ol>
	<li>Identify the Hyperic Server nodes in the cluster.</li>
	<li>Configure the load balancer to check the nodeStatus.hqu URL every 10 seconds. &nbsp;For example, in a 2-node cluster, if the the IP addresses of the nodes are 10.0.0.1 and 10.0.0.2, configure the load balancer to check these URLs every 10 seconds:
	<ul>
		<li><div class="preformatted panel" style="border-width: 1px;"><div class="preformattedContent panelContent">
<pre>http://hqadmin:hqadmin@10.0.0.1:7080/hqu/health/status/nodeStatus.hqu</pre>
</div></div></li>
		<li><div class="preformatted panel" style="border-width: 1px;"><div class="preformattedContent panelContent">
<pre>http://hqadmin:hqadmin@10.0.0.2:7080/hqu/health/status/nodeStatus.hqu</pre>
</div></div></li>
	</ul>
	</li>
	<li>Configure the load balancer to direct all traffic to the node whose status is <tt>master=true</tt>.</li>
</ol>


<h2><a name="ClusteringHypericServersforFailover-Step5ConfigureAgentstoCommunicatewithHypericServerCluster"></a>Step 5 - Configure Agents to Communicate with Hyperic Server Cluster</h2>

<p>The Hyperic Agents in your environment communicate with the Hyperic Server cluster through the load balancer. When you startup a newly installed agent, either supply the load balancer listen address and port interactively, or specify the connection information in agent.properties.</p>

<p>For existing agents, you can run <tt>hq-agent.sh setup</tt>, to force the setup dialog.</p>

<h2><a name="ClusteringHypericServersforFailover-Step6StarttheNodes"></a>Step 6 - Start the Nodes</h2>

<p>Start the Hyperic Servers.</p>

<h2><a name="ClusteringHypericServersforFailover-TroubleshootingaFailoverConfiguration"></a>Troubleshooting a Failover Configuration</h2>

<p>This section describes the most common sources of problems the failover configuration.</p>
<ul>
	<li>Multicast blocking ---T he cluster detection and cache peer detection relies on multicast. Make sure your router isn't blocking multicast packets; otherwise the Hyperic cluster will fail to initialize properly. It's also common for virtualization technologies like VMware and Xen to not enable multicast by default.</li>
</ul>


<ul>
	<li>Don't register agents using the loopback address &#8212; If you install an Hyperic Agent on the same machine as a Hyperic Server node, when you specify the IP address the server should use to contact the agent, don't specify loopback address (127.0.0.1).</li>
</ul>


<ul>
	<li>Alerts that were currently firing or in escalation were "lost" &#8212; A failover to another cluster node occurred in the middle of the alerts being fired or escalated. The alert state could be lost.</li>
</ul>




				    
                    			    </td>
		    </tr>
	    </table>
	    <table border="0" cellpadding="0" cellspacing="0" width="100%">
			<tr>
				<td height="12" background="http://support.hyperic.com/images/border/border_bottom.gif"><img src="images/border/spacer.gif" width="1" height="1" border="0"/></td>
			</tr>
		    <tr>
			    <td align="center"><font color="grey">Document generated by Confluence on May 21, 2012 08:34</font></td>
		    </tr>
	    </table>
    </body>
</html>
